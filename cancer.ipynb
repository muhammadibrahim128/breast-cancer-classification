{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "875cde7e-43a5-457f-a2ae-5eded9ccb537",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "53835083-f3f8-4653-bb8e-d3249a90e05d",
      "cell_type": "code",
      "source": "\n\nimport pandas as pd \nimport numpy as np \nfrom glob import glob\nimport cv2 \nimport matplotlib.pylab as plt \nimport random\nimport os\nimport re\n\n# view sample images\ncategories = ['benign', 'malignant', 'normal']\nbase_path = 'breast-ultrasound-images-dataset/'\nfor category in categories:\n    # Get all image paths for the current category\n    images_path = glob(os.path.join(base_path, category, \"*.png\"))\n    if images_path:\n        \n        sample_path = images_path[0]\n        sample_img = cv2.imread(sample_path)\n        \n        if sample_img is not None:\n            sample_img_rgb = cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB)\n            \n            # Find corresponding mask\n            mask_path = sample_path.replace('.png', '_mask.png')\n            \n            plt.figure(figsize=(12, 6))\n            plt.subplot(1, 2, 1)\n            plt.imshow(sample_img_rgb)\n            print(sample_img_rgb.shape)\n            plt.title(f\"{category}\")\n            if os.path.exists(mask_path):\n                mask_img = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n                plt.subplot(1, 2, 2)\n                plt.imshow(mask_img, cmap='gray')\n                print(mask_img.shape)\n                plt.title(f\"Sample {category} Mask\")\n            plt.axis('off')\n            plt.show()\n        else:\n            print(f\"Error: Unable to load image at {sample_path}\")\n    else:\n        print(f\"No images found in category: {category}\")\n\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "id": "7612a5b3-d172-4633-951c-c7b20a709aad",
      "cell_type": "code",
      "source": "# count number images\nfor category in categories:\n    print(f'sample path: {len(glob(os.path.join(base_path, category, \"*.png\")))} images in {category}')\n    images_path = glob(os.path.join(base_path, category, \"*.png\"))\n    print(images_path[0]) ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "620a0cdc-2fb6-42a2-ae99-710082656522",
      "cell_type": "code",
      "source": "# Resize images\nfor category in categories:\n    \n    images_path = glob(os.path.join(base_path, category, \"*.png\"))\n    \n    for i in images_path:\n        original = cv2.imread(i, 0)  \n        if original is not None:  \n            resized = cv2.resize(original, (256, 256))  \n            cv2.imwrite(i, resized)  \n            \n        else:\n            print(f\"Error: Unable to read image at {i}\")\n\n    print(f\"Resized images in category: {category}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "76e80c58-85fc-44d3-8a06-c8dab4b75fb3",
      "cell_type": "code",
      "source": "# verify all images are 256x256 \n\nfor category in categories:\n\n    images = glob(os.path.join(base_path, category, '*.png'))\n    \n    for image in images:\n        current_image = cv2.imread(image, cv2.IMREAD_UNCHANGED)  \n        \n        if current_image is not None: \n            current_shape = current_image.shape\n\n            if current_shape != (256, 256) and current_shape != (256, 256, 3):\n                print(f\"Image Path: {image}\")\n                print(f\"Shape: {current_shape}\")\n        else:\n            print(f\"Error: Unable to read image at {image}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "264872ce-7827-4ad6-aa66-a422da60e790",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "130c90c2-9696-4d8f-a8a7-fe1caccdbeea",
      "cell_type": "code",
      "source": "# Run thistraining code as separate .ipynb file \n\nfrom tqdm import tqdm\nimport sys\nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nfrom torch.utils.data import Dataset, DataLoader \nimport torchvision \nfrom torchvision import models, transforms \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score \n\n\n\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n\ndevice = torch.device(\"cpu\")\n\n\n\nDATASET_PATH = \"breast-ultrasound-images-dataset\"\nBENIGN_PATH = os.path.join(DATASET_PATH, \"benign\")\nMALIGNANT_PATH = os.path.join(DATASET_PATH, \"malignant\")\nNORMAL_PATH = os.path.join(DATASET_PATH, \"normal\")\n\n\nBATCH_SIZE = 16\nEPOCHS = 30\nLEARNING_RATE = 0.0001\nIMAGE_SIZE = 224  \nNUM_CLASSES = 3\nVALIDATION_SPLIT = 0.2\n\n\nclass_map = {\n    'benign': 0,\n    'malignant': 1,\n    'normal': 2\n}\nidx_to_class = {v: k for k, v in class_map.items()}\n\n# Check dataset structure\ndef inspect_dataset():\n    benign_imgs = glob(os.path.join(BENIGN_PATH, \"benign (*).png\"))\n    benign_masks = glob(os.path.join(BENIGN_PATH, \"benign (*)_mask.png\"))\n    malignant_imgs = glob(os.path.join(MALIGNANT_PATH, \"malignant (*).png\"))\n    malignant_masks = glob(os.path.join(MALIGNANT_PATH, \"malignant (*)_mask.png\"))\n    normal_imgs = glob(os.path.join(NORMAL_PATH, \"normal (*).png\"))\n    normal_masks = glob(os.path.join(NORMAL_PATH, \"normal (*)_mask.png\"))\n\n    print(f\"Benign images: {len(benign_imgs)}\")\n    print(f\"Benign masks: {len(benign_masks)}\")\n    print(f\"Malignant images: {len(malignant_imgs)}\")\n    print(f\"Malignant masks: {len(malignant_masks)}\")\n    print(f\"Normal images: {len(normal_imgs)}\")\n    print(f\"Normal masks: {len(normal_masks)}\")\n    return {\n        'benign': [img for img in benign_imgs if '_mask' not in img],\n        'malignant': [img for img in malignant_imgs if '_mask' not in img],\n        'normal': normal_imgs\n    }\n\n\nclass BreastUltrasoundDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None, use_masks=False):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n        self.use_masks = use_masks\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.use_masks:\n            mask_path = img_path.replace('.png', '_mask.png')\n            if os.path.exists(mask_path):\n                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n                mask = mask / 255.0  \n                img = img * mask[..., None]  \n        \n        if self.transform:\n            img = self.transform(img)\n        \n        return img, self.labels[idx]\n\ndef prepare_data(use_masks=False):\n    \n    class_images = inspect_dataset()\n    \n    all_images = []\n    all_labels = []\n    \n    for class_name, img_paths in class_images.items():\n        for img_path in img_paths:\n            all_images.append(img_path)\n            all_labels.append(class_map[class_name])\n    \n    \n    train_imgs, val_imgs, train_labels, val_labels = train_test_split(\n        all_images, all_labels, test_size=VALIDATION_SPLIT, \n        random_state=42, stratify=all_labels\n    )\n    \n    print(f\"Training set: {len(train_imgs)} images\")\n    print(f\"Validation set: {len(val_imgs)} images\")\n    \n    # Define transformations\n    train_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                         std=[0.229, 0.224, 0.225])\n    ])\n    \n    val_transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                             std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create datasets\n    train_dataset = BreastUltrasoundDataset(\n        train_imgs, train_labels, transform=train_transform, use_masks=use_masks\n    )\n    val_dataset = BreastUltrasoundDataset(\n        val_imgs, val_labels, transform=val_transform, use_masks=use_masks\n    )\n    \n    \n    train_loader = DataLoader(\n        train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n    )\n    val_loader = DataLoader(\n        val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n    )\n    \n    return train_loader, val_loader\n\n\ndef create_model(use_masks=False):\n    \n    model = models.densenet121(weights='IMAGENET1K_V1')\n    \n\n    \n    \n    num_ftrs = model.classifier.in_features\n    model.classifier = nn.Sequential(\n        nn.Dropout(0.2),\n        nn.Linear(num_ftrs, NUM_CLASSES)\n    )\n    \n    return model\n\n# Training function\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n    best_val_acc = 0.0\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'train_acc': [],\n        'val_acc': [],\n    }\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n        \n        # Training phase\n        model.train()\n        running_loss = 0.0\n        running_corrects = 0\n        \n        for inputs, labels in tqdm(train_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            # Zero the parameter gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            with torch.set_grad_enabled(True):\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n                \n                # Backward pass + optimize\n                loss.backward()\n                optimizer.step()\n            \n            # Statistics\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n        \n        epoch_loss = running_loss / len(train_loader.dataset)\n        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n        \n        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n        \n        history['train_loss'].append(epoch_loss)\n        history['train_acc'].append(epoch_acc.item())\n        if scheduler is not None:\n            scheduler.step(epoch_loss)\n\n        # Validation phase\n        model.eval()\n        running_loss = 0.0\n        running_corrects = 0\n        \n        for inputs, labels in tqdm(val_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            with torch.no_grad():\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n            \n            # Statistics\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n        \n        epoch_loss = running_loss / len(val_loader.dataset)\n        epoch_acc = running_corrects.double() / len(val_loader.dataset)\n        \n        print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n        \n        history['val_loss'].append(epoch_loss)\n        history['val_acc'].append(epoch_acc.item())\n        \n        \n        if epoch_acc > best_val_acc:\n            best_val_acc = epoch_acc\n            torch.save(model.state_dict(), 'best_densenet121_breast_ultrasound.pth')\n            print(f'Best model saved with accuracy: {best_val_acc:.4f}')\n    \n    return model, history\n\n\ndef plot_training_history(history):\n    plt.figure(figsize=(12, 5))\n    \n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_acc'])\n    plt.plot(history['val_acc'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    \n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history['train_loss'])\n    plt.plot(history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    \n    plt.tight_layout()\n    plt.savefig('training_history.png')\n    plt.show()\n\n\ndef evaluate_model(model, val_loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(val_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    conf_matrix = confusion_matrix(all_labels, all_preds)\n    class_report = classification_report(all_labels, all_preds, \n                                         target_names=list(class_map.keys()),\n                                         output_dict=True)\n    \n    print(f'Validation Accuracy: {accuracy:.4f}')\n    print('\\nConfusion Matrix:')\n    print(conf_matrix)\n    print('\\nClassification Report:')\n    print(classification_report(all_labels, all_preds, target_names=list(class_map.keys())))\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(10, 8))\n    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    tick_marks = np.arange(len(class_map))\n    plt.xticks(tick_marks, class_map.keys(), rotation=45)\n    plt.yticks(tick_marks, class_map.keys())\n    \n    fmt = 'd'\n    thresh = conf_matrix.max() / 2.\n    for i in range(conf_matrix.shape[0]):\n        for j in range(conf_matrix.shape[1]):\n            plt.text(j, i, format(conf_matrix[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n    \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.savefig('confusion_matrix.png')\n    plt.show()\n    \n    return accuracy, conf_matrix, class_report\n\n\ndef predict_image(model, image_path, use_masks=False):\n    model.eval()\n    \n    \n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    \n    if use_masks:\n        mask_path = image_path.replace('.png', '_mask.png')\n        if os.path.exists(mask_path):\n            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n            mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n            img = np.concatenate([img, mask], axis=2)\n    \n    \n    transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    img = transform(img).unsqueeze(0).to(device)\n    \n    # Make prediction\n    with torch.no_grad():\n        outputs = model(img)\n        _, preds = torch.max(outputs, 1)\n        probs = torch.nn.functional.softmax(outputs, dim=1)\n    \n    pred_class = idx_to_class[preds.item()]\n    pred_prob = probs[0][preds].item()\n    \n    return pred_class, pred_prob, probs.cpu().numpy()[0]\n\n# Main execution\nif __name__ == \"__main__\":\n    \n    USE_MASKS = True  # Set to False if you don't want to use mask images\n    \n  \n    train_loader, val_loader = prepare_data(use_masks=USE_MASKS)\n    \n    \n    model = create_model(use_masks=USE_MASKS)\n    model = model.to(device)\n    \n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    \n    \n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.1, patience=5\n    )\n    \n    \n    model, history = train_model(\n        model, train_loader, val_loader, criterion, optimizer, EPOCHS\n    )\n    \n    \n    plot_training_history(history)\n    \n    \n    model.load_state_dict(torch.load('best_densenet121_breast_ultrasound.pth'))\n    \n   \n    accuracy, conf_matrix, class_report = evaluate_model(model, val_loader)\n    \n    \n    \n    print(\"Training and evaluation completed!\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7142b8b4-0257-4892-8682-36cd5b62490f",
      "cell_type": "code",
      "source": "# convert model to light format, optimized for edge user \n\nimport os\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport numpy as np\nimport onnx\nimport onnx_tf\nimport tensorflow as tf\nfrom onnx_tf.backend import prepare\n\ndef create_model(num_classes=3, use_masks=False):\n    model = models.densenet121(weights=None)\n    \n    if use_masks:\n        original_conv = model.features.conv0\n        new_conv = nn.Conv2d(\n            6, original_conv.out_channels, \n            kernel_size=original_conv.kernel_size,\n            stride=original_conv.stride,\n            padding=original_conv.padding,\n            bias=original_conv.bias is not None\n        )\n        \n        model.features.conv0 = new_conv\n    \n    num_ftrs = model.classifier.in_features\n    model.classifier = nn.Sequential(\n        nn.Dropout(0.2),\n        nn.Linear(num_ftrs, num_classes)\n    )\n    \n    return model\n\ndef convert_pytorch_to_tflite(pth_model_path, output_dir, use_masks=False, num_classes=3):\n    \"\"\"\n    Convert PyTorch model (.pth) to TensorFlow Lite (.tflite)\n    \n    Args:\n        pth_model_path (str): Path to the .pth model file\n        output_dir (str): Directory to save the output models\n        use_masks (bool): Whether the model was trained with masks (6 channels input)\n        num_classes (int): Number of output classes\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(\"Loading PyTorch model...\")\n    \n    pytorch_model = create_model(num_classes=num_classes, use_masks=use_masks)\n    \n    pytorch_model.load_state_dict(torch.load(pth_model_path, map_location=torch.device('cpu')))\n    pytorch_model.eval()\n    \n    print(\"Converting to ONNX format...\")\n    onnx_path = os.path.join(output_dir, \"densenet121_model.onnx\")\n    \n    input_channels = 6 if use_masks else 3\n    dummy_input = torch.randn(1, input_channels, 224, 224)\n    \n    torch.onnx.export(\n        pytorch_model,                     \n        dummy_input,                       \n        onnx_path,                         \n        export_params=True,                \n        opset_version=11,                  \n        do_constant_folding=True,          \n        input_names=['input'],            \n        output_names=['output'],           \n        dynamic_axes={'input': {0: 'batch_size'},    \n                     'output': {0: 'batch_size'}}\n    )\n    \n    print(\"Verifying ONNX model...\")\n    onnx_model = onnx.load(onnx_path)\n    onnx.checker.check_model(onnx_model)\n    print(\"ONNX model is valid!\")\n    \n    print(\"Converting ONNX to TensorFlow...\")\n    tf_rep = prepare(onnx_model)\n    tf_model_path = os.path.join(output_dir, \"densenet121_tf_model\")\n    tf_rep.export_graph(tf_model_path)\n    \n    \n    print(\"Converting to TensorFlow Lite...\")\n    converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)\n    \n    \n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.target_spec.supported_types = [tf.float16]  \n    \n    \n    tflite_model = converter.convert()\n    \n    \n    tflite_path = os.path.join(output_dir, \"densenet121_model.tflite\")\n    with open(tflite_path, 'wb') as f:\n        f.write(tflite_model)\n    \n    print(f\"Conversion completed! TFLite model saved to {tflite_path}\")\n    \n    # Step 6: Verify the TFLite model (optional)\n    print(\"Verifying TFLite model...\")\n    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n    interpreter.allocate_tensors()\n    \n    # Get input and output details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    \n    print(\"Input details:\", input_details)\n    print(\"Output details:\", output_details)\n    \n    # Return paths to the generated models\n    return {\n        'onnx_path': onnx_path,\n        'tf_model_path': tf_model_path,\n        'tflite_path': tflite_path\n    }\n\nif __name__ == \"__main__\":\n    \n    PTH_MODEL_PATH = \"model.pth\"  \n    OUTPUT_DIR = \"converted_models\"  \n    USE_MASKS = False  \n    NUM_CLASSES = 3 \n    \n    \n    output_paths = convert_pytorch_to_tflite(\n        pth_model_path=PTH_MODEL_PATH,\n        output_dir=OUTPUT_DIR,\n        use_masks=USE_MASKS,\n        num_classes=NUM_CLASSES\n    )\n    \n    print(\"\\nConversion Summary:\")\n    print(f\"Original PyTorch model: {PTH_MODEL_PATH}\")\n    print(f\"ONNX model: {output_paths['onnx_path']}\")\n    print(f\"TensorFlow model: {output_paths['tf_model_path']}\")\n    print(f\"TFLite model: {output_paths['tflite_path']}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'ModuleNotFoundError'>",
          "evalue": "No module named 'torch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "8f0e8f30-ae4b-4803-8569-b92d4add6dbd",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}